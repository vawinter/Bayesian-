---
title: "STAT 597: Homework 3"
author: "V. Winter"
date: "3/16/2023"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---


**[Note: I tried a few different approaches for this homework, as is evident in each questions' respective answer.]**


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, warning=FALSE}
# set for reproducibility
set.seed(1234)

# Libraries
library(coda)
library(mvtnorm)
library(maptools)
library(maps)
library(mgcv)
library(glmnet)
library(MASS)
#library(nimble)
```

## Question 1
Let ùë¶ùëñ be the number of sick days that a person takes due to an illness, 
Let ùë•ùëñ be the number of months that person has been taking part in a 
treatment program.  

Data model : yi ~ Pois(exp{a + bxi})
a Poisson regression model with two regression parameters 
a and b.  
 
We will assume that both of these regression parameters have prior distributions 
that are Gaussian with mean zero and variance equal to 4.

Construct a Metropolis Hasting sampler that jointly proposes (a,b) from a
bivariate normal distribution with the current values of both parameters as the mean.
Begin with a proposal distribution that has a diagonal covariance matrix with
0.01 on both diagonal elements.

Use Shaby and Wells‚Äô log-adaptive tuning approach to adaptively tune the
proposal distribution, with adaptation happening every 100 MCMC iterations.
Run this until you are sure that your algorithm converges to the stationary
distribution.
 

```{r}
# Load in Question 1 data
x=c(8,14,11,7,32,8,28,21,27,15,26,13,19,22,15,
    12,15,7,9,15,26,22,16,12,6)

y=c(5,2,5,4,1,3,0,2,1,2,2,5,3,2,1,2,2,8,5,2,1,1,6,4,3)

## Define the log-likelihood function
loglike <- function(a, b, x, y) {
  return(sum(dpois(y, exp(a + b*x), log=TRUE)))
}

# Set up the prior distribution parameters
prior_mean <- c(0, 0)
prior_var <- diag(2)*4

# Set up the proposal distribution
proposal_var <- diag(2)*0.01

# Set up the initial values
a <- 0
b <- 0

# Set up the MCMC algorithm
niter <- 10000
accept <- 0
cov_mat <- proposal_var
tune_interval <- 100
tune_count <- 0
tune_iter <- tune_interval

# Sample storage
samples <- matrix(0, niter, 2)

# Run the MCMC algorithm
for(i in 1:niter) {
  # Generate a proposed value for a and b
  proposal <- mvrnorm(1, c(a, b), cov_mat)
  a_prop <- proposal[1]
  b_prop <- proposal[2]
  
  # Calculate the log-likelihoods for the current and proposed values
  loglike_curr <- loglike(a, b, x, y)
  loglike_prop <- loglike(a_prop, b_prop, x, y)
  
  # Calculate the log-prior densities for the current and proposed values
  logprior_curr <- dmvnorm(c(a, b), prior_mean, prior_var, log=TRUE)
  logprior_prop <- dmvnorm(c(a_prop, b_prop), prior_mean, prior_var, log=TRUE)
  
  # Calculate the log-acceptance ratio
  log_accept_ratio <- loglike_prop + logprior_prop - loglike_curr - logprior_curr
  
  # Accept or reject the proposed value
  if (log(runif(1)) < log_accept_ratio) {
    a <- a_prop
    b <- b_prop
    accept <- accept + 1
  }
  
  # Save the current values of a and b
  samples[i, 1] <- a
  samples[i, 2] <- b
  
  # Update the covariance matrix and tune the proposal distribution
  if (i == tune_iter) {
    # Calculate the acceptance rate
    acceptance_rate <- accept/tune_interval
    
    # Update the covariance matrix
    cov_mat <- cov(samples[(i-tune_interval+1):i,])
    
    # Tune the proposal distribution
    if (acceptance_rate < 0.2) {
      proposal_var <- proposal_var/2
    } else if (acceptance_rate > 0.3) {
      proposal_var <- proposal_var*2
    }
    
    # Reset the acceptance count
    accept <- 0
    
    # Increment the tune count and tune iteration
    tune_count <- tune_count
  }}  
    
# Print summary of results
cat("Mean of a:", mean(samples[,1]), "\n")
cat("Mean of b:", mean(samples[,2]), "\n")
cat("95% credible interval for a:", quantile(samples[,1], c(0.025, 0.975)), "\n")
cat("95% credible interval for b:", quantile(samples[,2], c(0.025, 0.975)), "\n")

### Accept ----
accept

# Plot results
par(mfrow=c(2,1))
plot(samples[,1], type="l", ylab="a", main="Traceplot of a")
plot(samples[,2], type="l", ylab="b", main="Traceplot of b")
```



### Explination:
This code is implementing a Metropolis-Hastings algorithm for Bayesian inference of the parameters of a Poisson regression model.

To start, the **loglike** function defines the log-likelihood function of the Poisson regression model, 
taking in the parameters a and b, as well as the predictor variable x and the response variable y. 
The function calculates the log-likelihood of the Poisson distribution with mean exp(a+b*x) and the observed values of y.

The prior distribution is specified with a mean vector **prior_mean** and a covariance matrix **prior_var**. The proposal distribution is a multivariate normal distribution with mean and covariance matrix given by c(a, b) and **proposal_var**, respectively.

The algorithm starts with initial values of a and b set to 0, and runs for a defined **niter** iterations. At each iteration, it generates a proposal for a and b from the proposal distribution, and calculates the log-likelihoods and log-prior densities for the current and proposed values. It then calculates the log-acceptance ratio, which is used to accept or reject the proposal. If the proposal is accepted, the values of a and b are updated, and if it is rejected, the current values are retained.

The algorithm saves the values of a and b at each iteration in a samples matrix. It also keeps track of the number of proposals accepted in accept, and updates the proposal distribution and covariance matrix every **tune_interval** iterations based on the acceptance rate.

This code utilizes a Shaby and Wells log adaptive tuning method, where the proposal distribution variance is adjusted during the MCMC based on the acceptance rate of the proposal. A low acceptance rate will result in the variance being decreased and a high acceptance rate results in the variance being increased.

The final output of the algorithm is the samples matrix, which can be used to calculate posterior summaries such as mean, median, and credible intervals of a and b.

# Question 2:
Specify a reasonable prior distribution for the following situations.

## part a.
Your data are Bernoulli distributed, with shared probability of success p.  Your goal for a prior distribution on p is that your prior is vague, giving equal probability for any valid value for p. 

### answer
p ~ Beta(1,1)
where p is the probability of success in a Bernoulli trial, and the parameters of the Beta distribution are both equal to 1. This prior has a closed-form expression for the posterior distribution, which is also 
a Beta distribution with updated parameters that depend on the number of successes and failures in the data.

## part b.
Your data are Bernoulli distributed, with shared probability of success p.  Your goal for a prior distribution on p is to specify a prior distribution that has a 90% probability that p is between 0 and 0.5, with a 10% probability that p is greater than 0.5. 

### answer
p ~ Beta(a, b)
where a and b are chosen so that 90% of the prior probability mass falls in the interval [0, 0.5] and 10% of the mass falls in the interval (0.5, 1]. *[Q: Can this be done with Empirical Bayes?]*

One option is to set a = 0.9 and b = 1.8.

The Beta(a,b) prior distribution has a pdf that is proportional to p^(a-1) * (1-p)^(b-1). 

## part c.
Your data are Bernoulli distributed, with shared probability of success p.  Your goal for a prior distribution on p is to specify a prior distribution that has a 50% probability that p is exactly 0, and a 50% probability that p is somewhere between 0 and 1, with equal probability given. 

### answer
A mixture distribution ("spike and slab") that assigns 0.5 to a Bernoulli distribution and and 50% probability to a continuous uniform distribution on the interval [0,1].
	f(p) = Bern(0.5)  + 0.5 * U(0,1)
where Bern(0.5) represents a point mass at 0.5 and U(0,1) represents a uniform distribution on the interval [0,1].
	
Could you also use a truncated normal distribution? as a prior for p here. Where you set the mean of the normal distribution to be less than zero so that the prior assigns a 50% probability to p being exactly 0. The variance of the normal distribution can be chosen to control the spread of the prior, then truncating the normal distribution at 0 and 1 so the prior assigns equal probability to p being between 0 and 1. (maybe?)
	
## part d.
Your data are classic linear regression data, with response and predictor variables. The regression parameter \beta for one parameter of interest has been well studied in the literature. Specify a prior for this distribution which allows for any real number, but which has a 95% prior probability of being between -0.2 and -0.1.

### answer
Empirical Bayes w. normal prior
A normal distribution with mean -0.15 and a chosen sd from the literature would allow for any real number
Ex: normal distribution with mean -0.15 and standard deviation 0.05, which would give a 95% prior probability of being between -0.2 and -0.1. 

## part e.
Your data come from a physical process where you know that your parameter must be between 0 and 2.  A previous study estimated the parameter as being very close to 2. Specify a prior that respects the required physical constraints, and also places a 75% probability that the parameter is between 1.8 and 2. 

### answer
a beta distribution with parameters alpha = 4 and beta = 1.25 would have a mean of 0.76 and a mode of 0.94, with 75% of the distribution falling between 1.8 and 2.

# Question 3
Linear Model with Ridge Regression priors
where i = 1, 2, ..., n are the lake observations, 
and beta0, beta1, ..., beta8 are the regression coefficients to be estimated.

To incorporate ridge regression prior on the regression coefficients, 
we assume that:
  
beta1, beta2, ..., beta8 ~ N(0, tau^2)
beta0 ~ N(0, 100)
where tau is a hyperparameter to be specified.

```{r, echo = TRUE}
# Load in data
load("../Homework3/lagos.Rdata")
source("../Homework3/mcmc.lm.rr.r")
#source("../Homework3/mcmc.lm.lasso.r")
source("../Homework3/ShabyWellsLogAdapt.r")
library(statmod)
```


```{r, warning=FALSE}
# Remove any NAs that may cause issues down the line (looking at you y.lag[1])
lagos <- na.omit(lagos)
## Create Indicator Vars
lagos$ag=0
lagos$ag[lagos$SurLand=="agricultural"] <- 1
lagos$forest=0
lagos$forest[lagos$SurLand=="forest"] <- 1

## Format the data
df <- data.frame(
  log.tp = log(lagos$tp),
  log.sec = log(lagos$secchi),
  x = lagos$lon,
  y = lagos$lat,
  x2 = lagos$lon^2,
  y2 = lagos$lat^2,
  ag = lagos$ag,
  forest = lagos$forest
)

## Format the data for 
df2 <- data.frame(
  log.tp = df$log.tp,
  log.sec = df$log.sec,
  x_all = df$x + df$x2,
  y_all = df$y + df$y2,
  ag = df$ag,
  forest = df$forest
)


# Set up response variable
y.lag=lagos[,5]

# Scale data frame
x_scale <- scale(df2)

# Run RR MCMC using EH code
lagos.out = mcmc.lm.rr(y = y.lag, 
                       X = x_scale, 
                       n.mcmc = 10000,
                       k2.start = 1,
                       beta.prior.var = 100,
                       s2.prior.var = var(y.lag),
                       k2.exp.rate = 1,
                       # Tuning parameter starting values
                       tune.k2 = 0.01,
                       tune.s2=.01,
                      # adapting every i'th iteration
                      adapt.iter=100)

# Create trace plots
par(mfrow=c(3,4))
for(k in 2:ncol(lagos.out)){
  plot(lagos.out[,k],main=colnames(lagos.out)[k],type="l")
  abline(v=0,col="red")
}

# Removing first 5000 for burn-in
burnin = 5000
effectiveSize(lagos.out[-c(1:burnin),])

for(k in 2:ncol(lagos.out)){
  hist(lagos.out[-c(1:burnin),k],main=colnames(lagos.out)[k])
  abline(v=0,col="red")
}

# Calculate the means
apply(lagos.out,2,mean)
# Calculate the credible intervals
t(apply(lagos.out, 2, function(x) quantile(x, c(0.025, 0.975))))

```

### Explination:
What I wanted this code to do was fit a linear regression model using MCMC to estimate the posterior distribution of the regression coefficients. The response variable is the log of the total nitrogen concentration (shown here as y.lag) in lake water. The predictor variables are the log of the total phosphorous concentration (log.tp), the log of the Secchi disk depth (log.sec), an indicator variable for whether the surrounding land is agricultural (ag), an indicator variable for whether the surrounding land is forested (forest), the longitude of the lake location (x), the latitude of the lake location (y), and the squared longitude and squared latitude (x2 and y2). The predictor vairables were scaled uring the **scale** funciton.

The priors for the regression coefficients are a ridge regression prior for the coefficients associated with the predictor variables, and a vague Gaussian prior for the intercept. The ridge regression prior adds a penalty term to the likelihood function that shrinks the coefficients towards zero, which can help to prevent overfitting.

I first tried using the package **nimble** to specify and compile the model and run an MCMC using the **nimbleMCMC** function. The MCMC algorithm was set to run for 20000 iterations, with the first 5000 iterations discarded as burn-in, and the remaining 15000 iterations used to estimate the posterior distribution of the regression coefficients. However, due to some compiling issues, this was scratched. 
I then tried using the function given in the **mcmc.lm.rr.r** file and was able to get this to successfully run. The posterior means and credible intervals for the parameters are then calculated using the apply and quantile functions.

# Question 4

Assume that you have the following count data, which are the number of calls to a help line
each hour.
The assumed model is that there are two latent ‚Äústates‚Äù, one with high call rate and one with
lower call rate. 

## part a. 
Show that you will need to use MH steps for ùúÜ0 and ùúÜ1, using the above model as
written. Implement an MCMC sampler to draw samples from the posterior distribution,
and report posterior means and 95% credible intervals of the mean number of calls in
the ‚Äúhigh‚Äù state which has rate (ùúÜ0 + ùúÜ1).

## WHY MCMC:
we can look at the conditional posterior distributions of these parameters given the data and other parameters in the model.
Starting with ùúÜ0, we have:

ùúÜ0 | ùë¶, ùúÜ1, ùëù ‚àº ùê∫ùëéùëöùëöùëé(1 + ùëõ/2, 0.1 + ‚àë(ùë¶ùë° ‚àí ùúÜ1ùë•ùë°)¬≤/2)

This posterior distribution has a gamma shape but is not conjugate to the prior distribution, so we cannot sample from it directly using Gibbs sampling. Instead, we can use MH steps to propose new values of ùúÜ0 and accept or reject them based on the ratio of the posterior densities at the proposed and current values of ùúÜ0.


```{r}
library(rjags)

# Define the data
y <- c(9, 15, 14, 5, 6, 4, 4, 4, 8, 0, 10, 22, 7, 2, 6, 2, 18, 9, 4, 2, 5, 7, 
       7, 5, 8, 7, 2, 15, 17, 7, 1, 4, 8, 5, 8, 9, 25, 6, 6, 4, 22, 3, 2, 5, 3,
       4, 8, 4, 17, 14)
T <- length(y)

# Define the JAGS model
model_string <- "
  model {
    # Priors
    lambda0 ~ dgamma(1, 0.1)
    lambda1 ~ dgamma(1, 0.1)
    p ~ dunif(0, 1)

    # Likelihood
    for (t in 1:T) {
      y[t] ~ dpois(lambda[t])
      lambda[t] <- lambda0 + lambda1 * x[t]
      x[t] ~ dbern(p)
    }
  }
"

# Prepare the data list for JAGS
data_list <- list(y = y, T = T)

# Set the JAGS parameters
params <- c("lambda0", "lambda1", "p", "x")

# Set the number of iterations and burn-in period
n_iter <- 5000
n_burnin <- 1000

# Initialize the MCMC chains
n_chains <- 3
jags_inits <- list(
  list(lambda0 = 10, lambda1 = 10, p = 0.5, x = rbinom(T, 1, 0.5)),
  list(lambda0 = 10, lambda1 = 10, p = 0.5, x = rbinom(T, 1, 0.5)),
  list(lambda0 = 10, lambda1 = 10, p = 0.5, x = rbinom(T, 1, 0.5))
)

# Run the MCMC sampler
jags_model <- jags.model(textConnection(model_string), data = data_list, inits = jags_inits, n.chains = n_chains)
jags_samples <- coda.samples(jags_model, variable.names = params, n.iter = n_iter, n.burnin = n_burnin, thin = 1)

# Calculate the posterior means and credible intervals
post_means <- apply(jags_samples[[1]], 2, mean)
post_cis <- t(apply(jags_samples[[1]], 2, function(x) quantile(x, c(0.025, 0.975))))
post_lambdas <- post_means[1] + post_means[2]

# Print the results
cat("Posterior mean of lambda_high:", post_lambdas, "\n")
post_cis

# Create trace plots
plot(jags_samples[, "lambda0"], main = "Trace plot for lambda0")
plot(jags_samples[, "lambda1"], main = "Trace plot for lambda1")
plot(jags_samples[, "p"], main = "Trace plot for p")



```

## What this code does:
Since I have continue nimble issues, I opted to use **rjags**. The code uses JAGS to implement an MCMC sampler using the Metropolis-Hastings algorithm to draw samples from the posterior distribution of the mean number of calls in the
"high" state of a Poisson model with two latent states.

The JAGS model draws samples from the posterior distribution of the mean number of calls in the "high" state which has a rate (ùúÜ0 + ùúÜ1). The observed data is defined as y. The model has priors for ùúÜ0, ùúÜ1, and p, which are gamma, gamma, and uniform distributions respectively. The likelihood is defined as a Poisson distribution, where the rate parameter (lambda) is defined as ùúÜ0 + ùúÜ1 * x[t], and x[t] is a Bernoulli distribution with parameter p.

The code then prepares the data list, sets the parameters, and specifies the number of iterations, burn-in period, and lastly initializes the MCMC chains using **jags.model** and **coda.samples** functions. The model then generate samples using the specified number of iterations, and then calculates the posterior means and credible intervals for the model parameters.

Finally, the code calculates the posterior mean of the mean number of calls in the "high" state by adding the posterior means of ùúÜ0 and ùúÜ1.

## part b. 
Under this new model, show that ùëõùë°0 and ùëõùë°1 have conjugate updates, and implement
an MCMC sampler to draw samples from the posterior distribution, and report posterior
means and 95% credible intervals of the mean number of calls in the ‚Äúhigh‚Äù state which
has rate (ùúÜ0 + ùúÜ1).

### Conjugate updates
First,  since I've been practicing my '*LaTeX*' math writing, the joint posterior distribution of the model can be written as:
  
  $$p(\boldsymbol{n}, \boldsymbol{\lambda_0}, \boldsymbol{\lambda_1}, \boldsymbol{p}|\boldsymbol{y}) \propto \prod_{t=1}^{T} \left( Pois(n_{t,0}|\lambda_0) Pois(n_{t,1}|\lambda_1 x_t) \right) Ber(p|0,1) 
  Gam(\lambda_0|1,0.1) Gam(\lambda_1|1,0.1) \prod_{t=1}^{T} Pois(y_t|n_{t,0} + n_{t,1})$$
  
The conditional posterior distribution of ùëõùë°0| the data is:
  
  $$p(n_{t,0}|\boldsymbol{n}{-t}, \boldsymbol{\lambda_0}, \boldsymbol{\lambda_1}, \boldsymbol{p}, \boldsymbol{y}) \propto Pois(n{t,0}|\lambda_0) Pois(y_t - n_{t,1}|\lambda_0)$$
  
which is a Poisson distribution with parameter $\lambda_0$, and the conditional posterior distribution of ùëõùë°1 is:
  
  $$p(n_{t,1}|\boldsymbol{n}{-t}, \boldsymbol{\lambda_0}, \boldsymbol{\lambda_1}, \boldsymbol{p}, \boldsymbol{y}) \propto Pois(n{t,1}|\lambda_1 x_t) Pois(y_t - n_{t,0}|\lambda_1 x_t)$$
  
which is also a Poisson distribution with parameter $\lambda_1 x_t$. 

Therefore, ùëõùë°0 and ùëõùë°1 have conjugate updates.


```{r}
# In JAGS, again
# Define the data
y <- c(9, 15, 14, 5, 6, 4, 4, 4, 8, 0, 10, 22, 7, 2, 6, 2, 18, 9, 4, 2, 5, 7, 7, 
       5, 8, 7, 2, 15, 17, 7, 1, 4, 8, 5, 8, 9, 25, 6, 6, 4, 22, 3, 2, 5, 3, 
       4, 8, 4, 17, 14)

T <- length(y)

# Define the JAGS model
model_string2 <- "model{
  # Priors
  lambda0 ~ dgamma(1, 0.1)
  lambda1 ~ dgamma(1, 0.1)
  p ~ dunif(0, 1)

  # Data augmentation
  for(t in 1:T) {
    y[t] ~ dpois(nt0[t] + nt1[t])
    nt0[t] ~ dpois(lambda0)
    nt1[t] ~ dpois(lambda1 * x[t])
    x[t] ~ dbern(p)
  }
}"

# Prepare the data list for JAGS
data_list2 <- list(y = y, T = T)

# Set the JAGS parameters
params2 <- c("lambda0", "lambda1", "p", "x")

# Set the number of iterations and burn-in period
n_iter2 <- 5000
n_burnin2 <- 1000

# Initialize the MCMC chains
n_chains2 <- 3
jags_inits2 <- list(
  list(lambda0 = 10, lambda1 = 10, p = 0.5, x = rbinom(T, 1, 0.5)),
  list(lambda0 = 10, lambda1 = 10, p = 0.5, x = rbinom(T, 1, 0.5)),
  list(lambda0 = 10, lambda1 = 10, p = 0.5, x = rbinom(T, 1, 0.5))
)

# Run the MCMC sampler
jags_model2 <- jags.model(textConnection(model_string2), data = data_list2, inits = jags_inits2, n.chains = n_chains2)
jags_samples2 <- coda.samples(jags_model2, variable.names = params2, n.iter = n_iter2, n.burnin = n_burnin2, thin = 1)

# Calculate the posterior means and credible intervals
post_means2 <- apply(jags_samples2[[1]], 2, mean)
post_cis2 <- t(apply(jags_samples2[[1]], 2, function(x) quantile(x, c(0.025, 0.975))))
post_lambdas2 <- post_means2[1] + post_means2[2]

# Print the results
cat("Posterior mean of lambda_high:", post_lambdas2, "\n")
cat("95% credible interval of lambda_0:", post_cis2[1], "\n")
post_cis2

# Create trace plots
plot(jags_samples2[, "lambda0"], main = "Trace plot for lambda0")
plot(jags_samples2[, "lambda1"], main = "Trace plot for lambda1")
plot(jags_samples2[, "p"], main = "Trace plot for p")


```

### Explination:
The model here assumes that the counts are generated by a Poisson distribution, with a rate parameter that depends on an underlying binary variable x and two unknown Poisson rates lambda0 and lambda1. The binary variable x is modeled as a Bernoulli distribution with an unknown probability p.

Similarly to above, the code first defines the data y and the total number of observations T. Then, it specifies the JAGS model, which includes prior distributions for the unknown parameters, a data augmentation step to model the latent variables nt0 and nt1, and the likelihood function for the observed data y.

After preparing the data list and setting the parameters, the code initializes the MCMC chains and runs the sampler using the **jags.model** and **coda.samples** functions. The posterior means and credible intervals for the parameters are then calculated using the apply and quantile functions. Finally, the code calculates the posterior distribution of lambda0 + lambda1 as post_lambdas <- post_means[1] + post_means[2]







