---
title: "STAT 597: Homework 3"
author: "V. Winter"
date: "3/16/2023"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---


**Note: I tried several different techniques for this homework, as is evident in each questions' respective answer).**


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, warning=FALSE}
# set for reproducibility
set.seed(1234)

# Libraries
library(coda)
library(mvtnorm)
library(maptools)
library(maps)
library(mgcv)
library(glmnet)
library(MASS)
#library(nimble)
```

## Question 1
Let ùë¶ùëñ be the number of sick days that a person takes due to an illness, 
Let ùë•ùëñ be the number of months that person has been taking part in a 
treatment program.  

Data model : yi ~ Pois(exp{a + bxi})
a Poisson regression model with two regression parameters 
a and b.  
 
We will assume that both of these regression parameters have prior distributions 
that are Gaussian with mean zero and variance equal to 4.

Construct a Metropolis Hasting sampler that jointly proposes (a,b) from a
bivariate normal distribution with the current values of both parameters as the mean.
Begin with a proposal distribution that has a diagonal covariance matrix with
0.01 on both diagonal elements.

Use Shaby and Wells‚Äô log-adaptive tuning approach to adaptively tune the
proposal distribution, with adaptation happening every 100 MCMC iterations.
Run this until you are sure that your algorithm converges to the stationary
distribution.
 

```{r}
# Load in Question 1 data
x=c(8,14,11,7,32,8,28,21,27,15,26,13,19,22,15,
    12,15,7,9,15,26,22,16,12,6)

y=c(5,2,5,4,1,3,0,2,1,2,2,5,3,2,1,2,2,8,5,2,1,1,6,4,3)

## Define the log-likelihood function
loglike <- function(a, b, x, y) {
  return(sum(dpois(y, exp(a + b*x), log=TRUE)))
}

# Set up the prior distribution parameters
prior_mean <- c(0, 0)
prior_var <- diag(2)*4

# Set up the proposal distribution
proposal_var <- diag(2)*0.01

# Set up the initial values
a <- 0
b <- 0

# Set up the MCMC algorithm
niter <- 10000
accept <- 0
cov_mat <- proposal_var
tune_interval <- 100
tune_count <- 0
tune_iter <- tune_interval

# Sample storage
samples <- matrix(0, niter, 2)

# Run the MCMC algorithm
for(i in 1:niter) {
  # Generate a proposed value for a and b
  proposal <- mvrnorm(1, c(a, b), cov_mat)
  a_prop <- proposal[1]
  b_prop <- proposal[2]
  
  # Calculate the log-likelihoods for the current and proposed values
  loglike_curr <- loglike(a, b, x, y)
  loglike_prop <- loglike(a_prop, b_prop, x, y)
  
  # Calculate the log-prior densities for the current and proposed values
  logprior_curr <- dmvnorm(c(a, b), prior_mean, prior_var, log=TRUE)
  logprior_prop <- dmvnorm(c(a_prop, b_prop), prior_mean, prior_var, log=TRUE)
  
  # Calculate the log-acceptance ratio
  log_accept_ratio <- loglike_prop + logprior_prop - loglike_curr - logprior_curr
  
  # Accept or reject the proposed value
  if (log(runif(1)) < log_accept_ratio) {
    a <- a_prop
    b <- b_prop
    accept <- accept + 1
  }
  
  # Save the current values of a and b
  samples[i, 1] <- a
  samples[i, 2] <- b
  
  # Update the covariance matrix and tune the proposal distribution
  if (i == tune_iter) {
    # Calculate the acceptance rate
    acceptance_rate <- accept/tune_interval
    
    # Update the covariance matrix
    cov_mat <- cov(samples[(i-tune_interval+1):i,])
    
    # Tune the proposal distribution
    if (acceptance_rate < 0.2) {
      proposal_var <- proposal_var/2
    } else if (acceptance_rate > 0.3) {
      proposal_var <- proposal_var*2
    }
    
    # Reset the acceptance count
    accept <- 0
    
    # Increment the tune count and tune iteration
    tune_count <- tune_count
  }}  
    
# Print summary of results
cat("Mean of a:", mean(samples[,1]), "\n")
cat("Mean of b:", mean(samples[,2]), "\n")
cat("95% credible interval for a:", quantile(samples[,1], c(0.025, 0.975)), "\n")
cat("95% credible interval for b:", quantile(samples[,2], c(0.025, 0.975)), "\n")

### Accept ----
accept

# Plot results
par(mfrow=c(2,1))
plot(samples[,1], type="l", ylab="a", main="Traceplot of a")
plot(samples[,2], type="l", ylab="b", main="Traceplot of b")
```



### Explination:
This code is implementing a Metropolis-Hastings algorithm for Bayesian inference of the parameters of a Poisson regression model.

To start, the 'loglike' function defines the log-likelihood function of the Poisson regression model, 
taking in the parameters a and b, as well as the predictor variable x and the response variable y. 
The function calculates the log-likelihood of the Poisson distribution with mean exp(a+b*x) and the observed values of y.

The prior distribution is specified with a mean vector 'prior_mean' and a covariance matrix 'prior_var'. The proposal distribution is a multivariate normal distribution with mean and covariance matrix given by c(a, b) and 'proposal_var', respectively.

The algorithm starts with initial values of a and b set to 0, and runs for a defined 'niter' iterations. At each iteration, it generates a proposal for a and b from the proposal distribution, and calculates the log-likelihoods and log-prior densities for the current and proposed values. It then calculates the log-acceptance ratio, which is used to accept or reject the proposal. If the proposal is accepted, the values of a and b are updated, and if it is rejected, the current values are retained.

The algorithm saves the values of a and b at each iteration in a samples matrix. It also keeps track of the number of proposals accepted in accept, and updates the proposal distribution and covariance matrix every 'tune_interval' iterations based on the acceptance rate.

This code utilizes a Shaby and Wells log adaptive tuning method, where the proposal distribution variance is adjusted during the MCMC based on the acceptance rate of the proposal. A low acceptance rate will result in the variance being decreased and a high acceptance rate results in the variance being increased.

The final output of the algorithm is the samples matrix, which can be used to calculate posterior summaries such as mean, median, and credible intervals of a and b.

# Question 2:
Specify a reasonable prior distribution for the following situations.

## part a.
Your data are Bernoulli distributed, with shared probability of success p.  Your goal for a prior distribution on p is that your prior is vague, giving equal probability for any valid value for p. 

### answer
p ~ Beta(1,1)
where p is the probability of success in a Bernoulli trial, and the parameters of the Beta distribution are both equal to 1. This prior has a closed-form expression for the posterior distribution, which is also 
a Beta distribution with updated parameters that depend on the number of successes and failures in the data.

## part b.
Your data are Bernoulli distributed, with shared probability of success p.  Your goal for a prior distribution on p is to specify a prior distribution that has a 90% probability that p is between 0 and 0.5, with a 10% probability that p is greater than 0.5. 

### answer
Beta(a,b)
where a and b are chosen such that 90% of the prior probability mass falls in the interval [0,0.5] and 10% of the mass falls in the interval (0.5,1].

One way to achieve this is to set a = 0.9 and b = 1.8, which corresponds to a prior mean of 0.33 and a prior variance of 0.023. This choice of parameters gives a prior probability of 0.9 that p is between 0 and 0.5, and a prior probability of 0.1 that p is greater than 0.5.

The Beta(a,b) prior distribution has a pdf that is proportional to p^(a-1) * (1-p)^(b-1). 

## part c.
Your data are Bernoulli distributed, with shared probability of success p.  Your goal for a prior distribution on p is to specify a prior distribution that has a 50% probability that p is exactly 0, and a 50% probability that p is somewhere between 0 and 1, with equal probability given. 

### answer
A mixture distribution ("spike and slap") that assigns 50% probability to a point mass at 0 and 50% probability to a continuous uniform distribution on the interval [0,1].
	f(p) = 0.5 * delta(0) + 0.5 * U(0,1)
where delta(0) represents a point mass at 0 and U(0,1) represents a uniform distribution on the interval [0,1].
	
Could you also use a truncated normal distribution? as a prior for p here. Where youset the mean of the normal distribution to be less than zero so that the prior assigns a 50% probability to p being exactly 0. The variance of the normal distribution can be chosen to control the spread of the prior, then truncating the normal distribution at 0 and 1 so the prior assigns equal probability to p being between 0 and 1. (maybe?)
	
## part d.
Your data are classic linear regression data, with response and predictor variables. The regression parameter \beta for one parameter of interest has been well studied in the literature. Specify a prior for this distribution which allows for any real number, but which has a 95% prior probability of being between -0.2 and -0.1.

### answer
Empirical Bayes w. normal prior
A normal distribution with mean -0.15 and a chosen sd from the literature would allow for any real number
Ex: normal distribution with mean -0.15 and standard deviation 0.05, which would give a 95% prior probability of being between -0.2 and -0.1. 

## part e.
Your data come from a physical process where you know that your parameter must be between 0 and 2.  A previous study estimated the parameter as being very close to 2. Specify a prior that respects the required physical constraints, and also places a 75% probability that the parameter is between 1.8 and 2. 

### answer
a beta distribution with parameters alpha = 4 and beta = 1.25 would have a mean of 0.76 and a mode of 0.94, with 75% of the distribution falling between 1.8 and 2.

# Question 3
Linear Model with Ridge Regression priors
where i = 1, 2, ..., n are the lake observations, 
and beta0, beta1, ..., beta8 are the regression coefficients to be estimated.

To incorporate ridge regression prior on the regression coefficients, 
we assume that:
  
beta1, beta2, ..., beta8 ~ N(0, tau^2)
beta0 ~ N(0, 100)
where tau is a hyperparameter to be specified.

```{r, echo = TRUE}
# Load in data
load("../Homework3/lagos.Rdata")
source("../Homework3/mcmc.lm.rr.r")
#source("../Homework3/mcmc.lm.lasso.r")
source("../Homework3/ShabyWellsLogAdapt.r")
library(statmod)
```


```{r, warning=FALSE}
## Create Indicator Vars
lagos$ag=0
lagos$ag[lagos$SurLand=="agricultural"] <- 1
lagos$forest=0
lagos$forest[lagos$SurLand=="forest"] <- 1

## Format the data for 
df <- data.frame(
  #int = rep(1,nrow(lagos)),
  log.tp = log(lagos$tp),
  log.sec = log(lagos$secchi),
  x = lagos$lon,
  y = lagos$lat,
  x2 = lagos$lon^2,
  y2 = lagos$lat^2,
  ag = lagos$ag,
  forest = lagos$forest
)

# Set up response variable
y.lag=lagos[,5]
# There is an NA value here that I think is causing issues:
y.lag[1]
# Setting this to be the mean value (probably not correct)
mean(y.lag)
y.lag[1] <-  951.8362

# Scale data frame
x_scale <- scale(df)

# Run RR mcmc using EH code
lagos.out = mcmc.lm.rr(y = y.lag, 
                       X = x_scale, 
                       n.mcmc = 10000,
                       k2.start = 1,
                       beta.prior.var = 100,
                       s2.prior.var = var(y.lag),
                       k2.exp.rate = 1,
                       # Tuning parameter starting values
                       tune.k2 = 0.01,
                       tune.s2=.01,
                      # adapting every i'th iteration
                      adapt.iter=100)

## trace plots
par(mfrow=c(3,4))
for(k in 2:ncol(lagos.out)){
  plot(lagos.out[,k],main=colnames(lagos.out)[k],type="l")
  abline(v=0,col="red")
}

# Removing first 5000 for burn-in
burnin = 5000
effectiveSize(lagos.out[-c(1:burnin),])

for(k in 2:ncol(lagos.out)){
  hist(lagos.out[-c(1:burnin),k],main=colnames(lagos.out)[k])
  abline(v=0,col="red")
}

# Calculate the means
apply(lagos.out,2,mean)
# Calculate the credible intervals
t(apply(lagos.out, 2, function(x) quantile(x, c(0.025, 0.975))))

```

### Explination:
What I wanted this code to do was fit a linear regression model using MCMC to estimate the posterior distribution of the regression coefficients. The response variable is the log of the total nitrogen concentration (shown here as y.lag) in lake water. The predictor variables are the log of the total phosphorous concentration (log.tp), the log of the Secchi disk depth (log.sec), an indicator variable for whether the surrounding land is agricultural (ag), an indicator variable for whether the surrounding land is forested (forest), the longitude of the lake location (x), the latitude of the lake location (y), and the squared longitude and squared latitude (x2 and y2). The predictor vairables were scaled uring the 'scale' funciton.

The priors for the regression coefficients are a ridge regression prior for the coefficients associated with the predictor variables, and a vague Gaussian prior for the intercept. The ridge regression prior adds a penalty term to the likelihood function that shrinks the coefficients towards zero, which can help to prevent overfitting.

I first tried using the package 'nimble' to specify and compile the model and run an MCMC using the 'nimbleMCMC' function. The MCMC algorithm was set to run for 20000 iterations, with the first 5000 iterations discarded as burn-in, and the remaining 15000 iterations used to estimate the posterior distribution of the regression coefficients. However, due to some compiling issues, this was scratched. 
I then tried using the function given in the 'mcmc.lm.rr.r' file and was able to get this to successfully run. The posterior means and credible intervals for the parameters are then calculated using the apply and quantile functions.

# Question 4

Assume that you have the following count data, which are the number of calls to a help line
each hour.
The assumed model is that there are two latent ‚Äústates‚Äù, one with high call rate and one with
lower call rate. 

## part a. 
Show that you will need to use MH steps for ùúÜ0 and ùúÜ1, using the above model as
written. Implement an MCMC sampler to draw samples from the posterior distribution,
and report posterior means and 95% credible intervals of the mean number of calls in
the ‚Äúhigh‚Äù state which has rate (ùúÜ0 + ùúÜ1).

## WHY MCMC:
Given the data y and the model specified above, the joint posterior distribution of the parameters is:

ùëù(ùúÜ0,ùúÜ1,ùëù,ùë•1,...,ùë•ùëõ|ùë¶1,...,ùë¶ùëõ) ‚àù ùëù(ùúÜ0)ùëù(ùúÜ1)ùëù(ùëù)‚àèùëñ=1ùëõùëùùëúùëñùë†(ùë¶ùëñ|ùúÜùë°)ùëù(ùë•ùëñ|ùëù)

The problem is that the parameters ùúÜ0 and ùúÜ1 are coupled in the model through the expression for ùúÜùë°. This means that it is not possible to directly sample from the conditional distributions ùëù(ùúÜ0|ùúÜ1,ùëù,ùë•,ùë¶) and ùëù(ùúÜ1|ùúÜ0,ùëù,ùë•,ùë¶), since the presence of ùúÜ1 and ùúÜ0 in ùúÜùë° means that the likelihood depends on both ùúÜ0 and ùúÜ1. Therefore, we need to use MH steps to sample from these conditional distributions.

These are not in closed form, and so we need to use MH steps to sample from them.
```{r}
# Define the data and prior distributions
y <- c(9, 15, 14, 5, 6, 4, 4, 4, 8, 0, 10, 22, 7, 2, 6, 2, 18, 
       9, 4, 2, 5, 7, 7, 5, 8, 7, 2, 15, 17, 7, 1, 4, 8, 5, 8, 
       9, 25, 6, 6, 4, 22, 3, 2, 5, 3, 4, 8, 4, 17, 14)

# Define the log-likelihood function
loglik <- function(lambda0, lambda1, p, y, n) {
  lambda <- lambda0 + lambda1 * p
  sum(dpois(y, lambda, log = TRUE))
}

# Define the log-prior function for lambda0
logprior_lambda0 <- function(lambda0, shape, rate) {
  dgamma(lambda0, shape, rate, log = TRUE)
}

# Define the log-prior function for lambda1
logprior_lambda1 <- function(lambda1, shape, rate) {
  dgamma(lambda1, shape, rate, log = TRUE)
}

# Set the initial values for the MCMC sampler
lambda0 <- rgamma(1, shape = 1, rate = 0.1)
lambda1 <- rgamma(1, shape = 1, rate = 0.1)
p <- runif(1)

# Set the parameters for the MCMC sampler
num_iter <- 5000
burn_in <- 1000
prop_sd_lambda0 <- 0.1
prop_sd_lambda1 <- 0.1
n <- length(y)
shape <- 1
rate <- 0.1
mean_gamma <- 10
var_gamma <- 100

# Initialize acceptance rates
accept_lambda0 <- 0
accept_lambda1 <- 0

# Run the MCMC sampler
for (i in 2:num_iter) {
  # Update p using Metropolis-Hastings
  p_prop <- rnorm(1, p[i-1], 0.1)
  log_alpha_p <- loglik(lambda0[i-1], lambda1[i-1], p_prop, y, n) - loglik(lambda0[i-1], lambda1[i-1], p[i-1], y, n)
  if (log(runif(1)) < log_alpha_p) {
    p[i] <- p_prop
  } else {
    p[i] <- p[i-1]
  }
  
  # Update lambda0 using Metropolis-Hastings
  lambda0_prop <- rnorm(1, lambda0[i-1], prop_sd_lambda0)
  log_alpha0 <- loglik(lambda0_prop, lambda1[i-1], p[i], y, n) - loglik(lambda0[i-1], lambda1[i-1], p[i], y, n) + 
    logprior_lambda0(lambda0_prop, shape, rate) - logprior_lambda0(lambda0[i-1], shape, rate)
  if (log(runif(1)) < log_alpha0) {
    lambda0[i] <- lambda0_prop
    accept_lambda0 <- accept_lambda0 + 1
  } else {
    lambda0[i] <- lambda0[i-1]
  }
  
  # Update lambda1 using Metropolis-Hastings
  lambda1_prop <- rnorm(1, lambda1[i-1], prop_sd_lambda1)
  log_alpha1 <- loglik(lambda0[i], lambda1_prop, p[i], y, n) - loglik(lambda0[i], lambda1[i-1], p[i], y, n) + 
    logprior_lambda1(lambda1_prop, shape, rate) - logprior_lambda1(lambda1[i-1], shape, rate)
  if (log(runif(1)) < log_alpha1) {
    lambda1[i] <- lambda1_prop
    accept_lambda1 <- accept_lambda1 + 1
  } else {
    lambda1[i] <- lambda1[i-1]
  }
}

# Find the posterior mean and 95% credible intervals for lambda_high
lambda_high <- lambda0 + lambda1
post_mean_lambda_high <- mean(lambda_high)
post_ci_lambda_high <- quantile(lambda_high, c(0.025, 0.975))

# Print the results
cat("Posterior mean of lambda_high: ", post_mean_lambda_high, "\n")
cat("95% credible interval for lambda_high: ", post_ci_lambda_high[1], "-", post_ci_lambda_high[2], "\n")


```

## What this code does:
The code implements an MCMC sampler using the Metropolis-Hastings algorithm to
draw samples from the posterior distribution of the mean number of calls in the
"high" state of a Poisson model with two latent states.

The model assumes that there are two latent states,
one with a high call rate and one with a lower call rate.
The observed data are the number of calls to a help line each hour.
The model assumes that the observed data follow a Poisson distribution with
parameter lambda(t), where lambda(t) is the sum of two parameters, lambda0 and lambda1 times a
binary indicator variable x(t). The indicator variable x(t) follows a Bernoulli
distribution with parameter p, which is itself assumed to follow a uniform
distribution on the interval (0, 1). The parameters lambda0 and lambda1 are assumed
to follow gamma distributions with shape parameters equal to 1 and rate
parameters equal to 0.1, which gives them means of 10 and variances of 100.

The Metropolis-Hastings algorithm is used to draw samples from the
joint posterior distribution of the parameters lambda0, lambda1, and p. The algorithm
iteratively generates proposals for the parameters and accepts or rejects them
based on the posterior density of the proposed values relative to the current values.

The code initializes the parameters lambda0, lambda1, and p and sets the
number of MCMC iterations. It then iteratively generates proposals for
the parameters and evaluates their posterior density using the observed data
and the model specification. The proposal distributions are chosen to be normal
distributions centered at the current values of the parameters, with standard
deviations chosen to ensure reasonable acceptance rates. The code stores the
accepted parameter values and reports the posterior means and 95% credible
intervals for the mean number of calls in the "high" state.

## part b. 
Under this new model, show that ùëõùë°0 and ùëõùë°1 have conjugate updates, and implement
an MCMC sampler to draw samples from the posterior distribution, and report posterior
means and 95% credible intervals of the mean number of calls in the ‚Äúhigh‚Äù state which
has rate (ùúÜ0 + ùúÜ1).

### Conjugate updates
First,  since I've been practicing my '*LaTeX*' math writing, the joint posterior distribution of the model can be written as:
  
  $$p(\boldsymbol{n}, \boldsymbol{\lambda_0}, \boldsymbol{\lambda_1}, \boldsymbol{p}|\boldsymbol{y}) \propto \prod_{t=1}^{T} \left( Pois(n_{t,0}|\lambda_0) Pois(n_{t,1}|\lambda_1 x_t) \right) Ber(p|0,1) 
  Gam(\lambda_0|1,0.1) Gam(\lambda_1|1,0.1) \prod_{t=1}^{T} Pois(y_t|n_{t,0} + n_{t,1})$$
  
The conditional posterior distribution of ùëõùë°0| the data is:
  
  $$p(n_{t,0}|\boldsymbol{n}{-t}, \boldsymbol{\lambda_0}, \boldsymbol{\lambda_1}, \boldsymbol{p}, \boldsymbol{y}) \propto Pois(n{t,0}|\lambda_0) Pois(y_t - n_{t,1}|\lambda_0)$$
  
which is a Poisson distribution with parameter $\lambda_0$, and the conditional posterior distribution of ùëõùë°1 is:
  
  $$p(n_{t,1}|\boldsymbol{n}{-t}, \boldsymbol{\lambda_0}, \boldsymbol{\lambda_1}, \boldsymbol{p}, \boldsymbol{y}) \propto Pois(n{t,1}|\lambda_1 x_t) Pois(y_t - n_{t,0}|\lambda_1 x_t)$$
  
which is also a Poisson distribution with parameter $\lambda_1 x_t$. 

Therefore, ùëõùë°0 and ùëõùë°1 have conjugate updates.


```{r}
library(rjags)

# Define the data
y <- c(9, 15, 14, 5, 6, 4, 4, 4, 8, 0, 10, 22, 7, 2, 6, 2, 18, 9, 4, 2, 5, 7, 7, 5, 8, 7, 2, 15, 17, 7, 1, 4, 8, 5, 8, 9, 25, 6, 6, 4, 22, 3, 2, 5, 3, 4, 8, 4, 17, 14)

T <- length(y)

# Define the JAGS model
model_string <- "model{
  # Priors
  lambda_0 ~ dgamma(1, 0.1)
  lambda_1 ~ dgamma(1, 0.1)
  p ~ dunif(0, 1)

  # Data augmentation
  for(t in 1:T) {
    y[t] ~ dpois(nt0[t] + nt1[t])
    nt0[t] ~ dpois(lambda_0)
    nt1[t] ~ dpois(lambda_1 * x[t])
    x[t] ~ dbern(p)
  }
}"

# Prepare the data list for JAGS
data_list <- list(y = y, T = T)

# Set the JAGS parameters
params <- c("lambda_0", "lambda_1", "p", "x")

# Set the number of iterations and burn-in period
n_iter <- 5000
n_burnin <- 1000

# Initialize the MCMC chains
n_chains <- 3
jags_inits <- list(
  list(lambda_0 = 10, lambda_1 = 10, p = 0.5, x = rbinom(T, 1, 0.5)),
  list(lambda_0 = 10, lambda_1 = 10, p = 0.5, x = rbinom(T, 1, 0.5)),
  list(lambda_0 = 10, lambda_1 = 10, p = 0.5, x = rbinom(T, 1, 0.5))
)

# Run the MCMC sampler
jags_model <- jags.model(textConnection(model_string), data = data_list, inits = jags_inits, n.chains = n_chains)
jags_samples <- coda.samples(jags_model, variable.names = params, n.iter = n_iter, n.burnin = n_burnin, thin = 1)

# Calculate the posterior means and credible intervals
post_means <- apply(jags_samples[[1]], 2, mean)
post_cis <- t(apply(jags_samples[[1]], 2, function(x) quantile(x, c(0.025, 0.975))))
post_lambdas <- post_means[1] + post_means[2]

# Print the results
cat("Posterior mean of lambda_0:", post_means[1], "\n")
cat("95% credible interval of lambda_0:", post_cis[1], "\n")

```

### Explination:
The model assumes that the counts are generated by a Poisson distribution, with a rate parameter that depends on an underlying binary variable x and two unknown Poisson rates lambda_0 and lambda_1. The binary variable x is modeled as a Bernoulli distribution with an unknown probability p.

The code first defines the data y and the total number of observations T. Then, it specifies the JAGS model as a text string in the variable model_string. This model includes prior distributions for the unknown parameters, a data augmentation step to model the latent variables nt0 and nt1, and the likelihood function for the observed data y.

Since I have continue nimble isues, I opted to use 'rjags'. After preparing the data list and setting the JAGS parameters, the code initializes the MCMC chains and runs the sampler using the 'jags.model' and 'coda.samples' functions. The posterior means and credible intervals for the parameters are then calculated using the apply and quantile functions. Finally, the code calculates the posterior distribution of lambda_0 + lambda_1 as post_lambdas <- post_means[1] + post_means[2]


```




